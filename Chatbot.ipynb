{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we handcraft a transformer model from scratch using PyTorch. Transformer model is based on the idea of 'self-attention' which allows it to extract a very rich contextual representation of the words in the sentences. Those vectorial representations are then used in different NLP tasks. Soon after the invention of the original transformer architecture, BERT and GPT models were derived from the transformer encoder or decoder for transfer learning in various NLP tasks. The original transformer containing a encoder and a decoder was for sequence-to-sequence (seq2seq) tasks like language translation. Here we apply the original transformer achitecture to build another seq2seq model, a toy conversational chatbot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsML57Id8g4J"
   },
   "source": [
    "## Data preparation & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1640667650046,
     "user": {
      "displayName": "Dennis X",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07673593424094128995"
     },
     "user_tz": 300
    },
    "id": "Mb-BmSXK0od4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is the tiny toy corpus that the transformer model is going to be trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = [\n",
    "             [\"Hello i am jiayu. What is your name?\", \"I am Dennis. Nice to meet you.\"],\n",
    "             [\"How was the party yesterday?\", \"It was super nice. Everybody had fun.\"],\n",
    "             [\"When do you usually go to the office in the morning\", \"I usually go to the office at 9 but I was late this morning.\"],\n",
    "             [\"Do you want to meet in the campus at 10 pm tonight?\", \"okay that sounds good. I will meet you there tonight.\"],\n",
    "             [\"Does this Sunday works for everyone?\", \"Yes, it works perferct for me!\"],\n",
    "             [\"What is your favourite book of all time?\", \"Three body problem. I love it so much and I have read it twice!\"],\n",
    "             [\"What do you love doing for fun at spare time?\", \"I play soccer and go hiking and I sometimes go swimming too.\"],\n",
    "             [\"How was the exam last week? Was it hard?\", \"It was quite hard actually. I didn't do it well.\"],\n",
    "             [\"Hope you could enjoy your trip next week!\", \"Thanks! I'm sure I will!\"],\n",
    "             [\"Is it gonna rain later today?\", \"Yes I think so. Don't forget your umbrella!\"],\n",
    "             [\"So, what are your plans for this weekend?\", \"I don’t know. Do you want to get together or something?\"],\n",
    "             [\"I don’t know. Do you want to get together or something?\", \"How about going to see a movie?\"],\n",
    "             [\"How about going to see a movie?\", \"That sounds like a good idea. Maybe we should go out to eat beforehand.\"],\n",
    "             [\"That sounds like a good idea. Maybe we should go out to eat beforehand.\", \"It is fine with me. Where do you want to meet?\"],\n",
    "             [\"It is fine with me. Where do you want to meet?\", \"Let’s meet at Summer Pizza House. I have not gone there for a long time.\"],\n",
    "             [\"When should we meet?\", \"Well, the movie is shown at 2 pm. How about we meet at 1 pm?\"],\n",
    "             [\"What is she doing these days?\", \"She graduated last June, and she will start her teaching career next week.\"],\n",
    "             [\"Hi, I haven't seen you in a while.\", \"Yes, I know it has been a long time!\"],\n",
    "             [\"What have you been doing lately?\", \"I have been going to graduate school.\"],\n",
    "             [\"What are you majoring in?\", \"I am studying theoretical physics. It is hard!\"],\n",
    "             [\"Okay nice to see you today! Bye!\", \"Yeah me too! See you later, bye!\"],\n",
    "]\n",
    "train_raw = pd.DataFrame(train_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanse the data using the class 'S2STextPreprocessor' written in the file 'preprocessing.py'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "|Before cleansing...|\n",
      "---------------------\n",
      "There are 200 tokens\n",
      "---------------------\n",
      "|Cleansing corpus...|\n",
      "---------------------\n",
      "--------------------\n",
      "|After cleansing...|\n",
      "--------------------\n",
      "There are 156 tokens\n",
      "22.00% tokens were cleansed\n"
     ]
    }
   ],
   "source": [
    "print('-'*21 + '\\n|Before cleansing...|\\n' + '-'*21)\n",
    "vocab = corpus_to_vocab(train_raw)\n",
    "num_tokens_before = len(vocab)\n",
    "print(f\"There are {num_tokens_before} tokens\")\n",
    "\n",
    "print('-'*21 + '\\n|Cleansing corpus...|\\n' + '-'*21)\n",
    "preprocessor = S2STextPreprocessor(to_keep = \"\", max_seq_length = 20)\n",
    "train = preprocessor.cleanse_corpus(train_raw)\n",
    "\n",
    "print('-'*20 + '\\n|After cleansing...|\\n' + '-'*20)\n",
    "vocab = corpus_to_vocab(train)\n",
    "num_tokens_after = len(vocab)\n",
    "print(f\"There are {num_tokens_after} tokens\")\n",
    "print(f\"{100 * (num_tokens_before - num_tokens_after) / num_tokens_before:.2f}% tokens were cleansed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello i am jiayu  what is your name</td>\n",
       "      <td>i am dennis  nice to meet you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how was the party yesterday</td>\n",
       "      <td>it was super nice  everybody had fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>when do you usually go to the office in the mo...</td>\n",
       "      <td>i usually go to the office at 9 but i was late...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do you want to meet in the campus at 10 pm ton...</td>\n",
       "      <td>okay that sounds good  i will meet you there t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>does this sunday works for everyone</td>\n",
       "      <td>yes  it works perferct for me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>what is your favourite book of all time</td>\n",
       "      <td>three body problem  i love it so much and i ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>what do you love doing for fun at spare time</td>\n",
       "      <td>i play soccer and go hiking and i sometimes go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>how was the exam last week  was it hard</td>\n",
       "      <td>it was quite hard actually  i did not do it well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hope you could enjoy your trip next week</td>\n",
       "      <td>thanks  i am sure i will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>is it going to rain later today</td>\n",
       "      <td>yes i think so  do not forget your umbrella</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0               hello i am jiayu  what is your name    \n",
       "1                       how was the party yesterday    \n",
       "2  when do you usually go to the office in the mo...   \n",
       "3  do you want to meet in the campus at 10 pm ton...   \n",
       "4               does this sunday works for everyone    \n",
       "5           what is your favourite book of all time    \n",
       "6      what do you love doing for fun at spare time    \n",
       "7           how was the exam last week  was it hard    \n",
       "8          hope you could enjoy your trip next week    \n",
       "9                   is it going to rain later today    \n",
       "\n",
       "                                                   1  \n",
       "0                     i am dennis  nice to meet you   \n",
       "1              it was super nice  everybody had fun   \n",
       "2  i usually go to the office at 9 but i was late...  \n",
       "3  okay that sounds good  i will meet you there t...  \n",
       "4                     yes  it works perferct for me   \n",
       "5  three body problem  i love it so much and i ha...  \n",
       "6  i play soccer and go hiking and i sometimes go...  \n",
       "7  it was quite hard actually  i did not do it well   \n",
       "8                          thanks  i am sure i will   \n",
       "9       yes i think so  do not forget your umbrella   "
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, building vocabulary and preparing the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the preprocessor on the cleansed corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit_on_corpus(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent words:\n",
      "[('i', 20), ('you', 15), ('to', 15), ('it', 12), ('do', 11), ('is', 8), ('meet', 8), ('what', 7), ('the', 7), ('a', 7)]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "The word-to-index map:\n",
      "PAD: 0\n",
      "SOS: 1\n",
      "EOS: 2\n",
      "i: 3\n",
      "you: 4\n",
      "to: 5\n",
      "it: 6\n",
      "do: 7\n",
      "is: 8\n",
      "meet: 9\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "print(f\"The most frequent words:\\n{preprocessor.sorted_vocab[:10]}\")\n",
    "print(\"~\"*100)\n",
    "print(\"The word-to-index map:\")\n",
    "for word, idx in list(preprocessor.word2idx.items())[:10]:\n",
    "    print(f\"{word}: {idx}\")\n",
    "print(\"~\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out how many unique tokens we found in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 159 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# Word -> integer mapping\n",
    "print('Found %i unique tokens' %len(preprocessor.word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(preprocessor.num_words)\n",
    "print(preprocessor.max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build a transformer from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code up the transformer by subclassing ```nn.Module```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MultiHeadAttention block, which is going to be used in both encoder and decoder blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = emb_dim // num_heads\n",
    "\n",
    "        # Ensure that we have exactly emb_dim = head_dim * num_heads\n",
    "        assert emb_dim % num_heads == 0, \"emb_dim is not divisible by num_head!\"\n",
    "\n",
    "        self.q_linear = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "        self.k_linear = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "        self.v_linear = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "        self.fc_out = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask = None, approach = 0):\n",
    "        batch_size = queries.shape[0]\n",
    "        q_len, k_len, v_len = queries.shape[1], keys.shape[1], values.shape[1]  # k_len == v_len holds true for any cases\n",
    "\n",
    "        # Split embedding into num_heads pieces\n",
    "        queries = queries.reshape(batch_size, q_len, self.num_heads, self.head_dim)\n",
    "        keys = keys.reshape(batch_size, k_len, self.num_heads, self.head_dim)\n",
    "        values = values.reshape(batch_size, v_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        queries = self.q_linear(queries)\n",
    "        keys = self.k_linear(keys)\n",
    "        values = self.v_linear(values)\n",
    "        \n",
    "        # queries: (batch_size, q_len, num_heads, head_dim)\n",
    "        # keys: (batch_size, k_len, num_heads, head_dim)\n",
    "        # energies: (batch_size, num_heads, q_len, k_len)\n",
    "        if approach == 0:\n",
    "            energies = torch.matmul(queries.transpose(1, 2), keys.transpose(1, 2).transpose(-1, -2)) / math.sqrt(self.emb_dim)\n",
    "        else:\n",
    "            energies = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) / math.sqrt(self.emb_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            energies = energies.masked_fill(mask == 0, 1e-20)\n",
    "\n",
    "        # Apply softmax to the key dimension\n",
    "        attention_weights = torch.softmax(energies, dim = 3) \n",
    "\n",
    "        # attention_weights: (batch_size, num_heads, q_len, k_len)\n",
    "        # values: (batch_size, v_len, num_heads, head_dim)\n",
    "        # out: (batch_size, q_len, num_heads, head_dim)\n",
    "        if approach == 0:\n",
    "            out = torch.matmul(attention_weights, values.transpose(1, 2)).transpose(1, 2)\n",
    "        else:\n",
    "            out = torch.einsum(\"nhqt,nthd->nqhd\", [attention_weights, values])\n",
    "\n",
    "        # Concatenate the representations in all heads back together\n",
    "        out = out.reshape(batch_size, q_len, self.emb_dim)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One encoder block (TransformerBlock)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, forward_expansion, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(emb_dim, forward_expansion * emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * emb_dim, emb_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask = None, approach = 0):\n",
    "        attentions = self.attention(queries, keys, values, mask = mask, approach = approach)\n",
    "        x = self.norm1(attentions + queries)  # Adding skip connection\n",
    "        x = self.dropout(x)\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(forward + x)  # Adding skip connection\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 512])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_angles(pos, k, d):\n",
    "    \"\"\"\n",
    "    Get the angles for the positional encoding\n",
    "    \n",
    "    Arguments:\n",
    "        pos -- Column vector containing the positions [[0], [1], ...,[N-1]]\n",
    "        k --   Row vector containing the dimension span [[0, 1, 2, ..., d-1]]\n",
    "        d(integer) -- Encoding size\n",
    "    \n",
    "    Returns:\n",
    "        angles -- (pos, d) numpy array \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get i from dimension span k\n",
    "    i = k // 2\n",
    "    # Calculate the angles using pos, i and d\n",
    "    angles = pos / (10000 ** (2 * i / d))\n",
    "    \n",
    "    return angles\n",
    "\n",
    "def get_positional_encoding(positions, d):\n",
    "    \"\"\"\n",
    "    Precomputes a matrix with all the positional encodings \n",
    "    \n",
    "    Arguments:\n",
    "        positions (int) -- Maximum number of positions to be encoded \n",
    "        d (int) -- Encoding size \n",
    "    \n",
    "    Returns:\n",
    "        pos_encoding -- (1, position, d_model) A matrix with the positional encodings\n",
    "    \"\"\"\n",
    "    # Initialize a matrix angle_rads of all the angles \n",
    "    angle_rads = get_angles(np.array(range(positions)).reshape(-1, 1),\n",
    "                            np.array(range(d)).reshape(1, -1),\n",
    "                            d)\n",
    "  \n",
    "    # Apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # Apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return torch.from_numpy(pos_encoding).float()\n",
    "\n",
    "\"\"\"\n",
    "Test positional encoding\n",
    "\"\"\"\n",
    "pos_encoding = get_positional_encoding(50, 512)\n",
    "\n",
    "print(pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('d')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder that contains positional encoding and several encoder blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_dim, num_layers, num_heads, forward_expansion, dropout, max_seq_len, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.positional_encoding = get_positional_encoding(max_seq_len, emb_dim)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(emb_dim, num_heads, forward_expansion, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, seqs, embedded, mask, approach = 0):\n",
    "        batch_size, seq_len = seqs.shape\n",
    "        embedded = embedded + self.positional_encoding[:, :seq_len, :].to(self.device)\n",
    "        out = self.dropout(embedded)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask = mask, approach = approach)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One decoder block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, forward_expansion, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.transformer_block = TransformerBlock(emb_dim, num_heads, forward_expansion, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embedded_target_seqs, keys, values, source_mask, target_mask, approach = 0):\n",
    "        attentions = self.attention(embedded_target_seqs, embedded_target_seqs, embedded_target_seqs, mask = target_mask, approach = approach)\n",
    "        x = self.norm(attentions + embedded_target_seqs)  # Adding skip connection\n",
    "        queries = self.dropout(x)\n",
    "        out = self.transformer_block(queries, keys, values, mask = source_mask, approach = approach)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder that contains positional encoding and several decoder blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, emb_dim, num_layers, num_heads, forward_expansion, dropout, max_seq_len, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.positional_encoding = get_positional_encoding(max_seq_len, emb_dim)\n",
    "        self.layers = nn.ModuleList([DecoderBlock(emb_dim, num_heads, forward_expansion, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, seqs, embedded, enc_out, source_mask, target_mask, approach = 0):\n",
    "        batch_size, seq_len = seqs.shape\n",
    "        embedded = embedded + self.positional_encoding[:, :seq_len, :].to(self.device)\n",
    "        out = self.dropout(embedded)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, enc_out, enc_out, source_mask, target_mask, approach = approach) \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final transformer model. Note that because the inputs and outputs are both in English we use one single vocabulary that contains all words in both the input and output sentences for simplicity. Therefore, we also use the same embedding layer for both the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 pad_idx,\n",
    "                 emb_dim = 256, \n",
    "                 num_layers = 6, \n",
    "                 forward_expansion = 4, \n",
    "                 num_heads = 8, \n",
    "                 dropout = 0.1,\n",
    "                 device = 'cpu',\n",
    "                 max_seq_len = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.pad_idx = pad_idx\n",
    "        self.embedder = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.encoder = Encoder(emb_dim, num_layers, num_heads, forward_expansion, dropout, max_seq_len, device)\n",
    "        self.decoder = Decoder(emb_dim, num_layers, num_heads, forward_expansion, dropout, max_seq_len, device)\n",
    "        self.fc_out = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def get_source_mask(self, seqs):\n",
    "        # source_mask: (batch_size, 1, 1, seq_len)\n",
    "        source_mask = (seqs != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return source_mask.to(self.device)\n",
    "\n",
    "    def get_target_mask(self, seqs):\n",
    "        batch_size, seq_len = seqs.shape\n",
    "        # target_mask: (batch_size, 1, seq_len, seq_len)\n",
    "        target_mask = torch.tril(torch.ones([seq_len, seq_len])).expand(batch_size, 1, seq_len, seq_len)\n",
    "        return target_mask.to(self.device)\n",
    "    \n",
    "    def forward(self, source_seqs, target_seqs, approach = 0):\n",
    "        source_mask = self.get_source_mask(source_seqs)\n",
    "        target_mask = self.get_target_mask(target_seqs)\n",
    "        source_embedded = self.embedder(source_seqs)\n",
    "        target_embedded = self.embedder(target_seqs)\n",
    "        encoded_source_seqs = self.encoder(source_seqs, source_embedded, source_mask, approach)\n",
    "        out = self.decoder(target_seqs, target_embedded, encoded_source_seqs, source_mask, target_mask, approach)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(preprocessor.num_tokens, 0, 128, device = device, max_seq_len = preprocessor.max_seq_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 14])\n",
      "torch.Size([5, 16, 159])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(preprocessor.pad_minibatch(preprocessor.sequences[0][:5])).shape)\n",
    "print(model(torch.tensor(preprocessor.pad_minibatch(preprocessor.sequences[0][:5])), torch.tensor(preprocessor.pad_minibatch(preprocessor.sequences[1][:5]))).shape)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# BUILDING MODEL ###################\n",
    "import time\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first = True, padding_value = 0)\n",
    "    yy_pad = pad_sequence(yy, batch_first = True, padding_value = 0)\n",
    "\n",
    "    return xx_pad, yy_pad\n",
    "\n",
    "def experiment(train_dataset, args):\n",
    "    model = Transformer(preprocessor.num_tokens, 0, 128, 3, 4, 4, device = device, max_seq_len = preprocessor.max_seq_length)\n",
    "\n",
    "    print(model)\n",
    "    \n",
    "    # ****** Copy model to device ****** #\n",
    "    model.to(device)\n",
    "\n",
    "    # ====== Loss function ====== #\n",
    "    lossfunction = nn.CrossEntropyLoss(reduction = 'sum', ignore_index = 0) \n",
    "\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = 0.001)\n",
    "\n",
    "    # ====== Data collection ====== #\n",
    "    list_epoch = [] \n",
    "    list_train_loss = []\n",
    "    \n",
    "    best_loss = 10000\n",
    "\n",
    "    # ====== Loop ====== #\n",
    "    for epoch in range(args.epoch):  \n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        # ====== Train ====== #\n",
    "        model.train() # Set the model be 'train mode' \n",
    "        train_loss = 0 # to sum up each batch\n",
    "        normalization = 0\n",
    "        \n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = args.batch_size, shuffle = True, collate_fn = pad_collate)\n",
    "        \n",
    "        for batch_X, batch_y in train_dataloader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # ****** Transfer data to device ****** #\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            pred_y = model(batch_X, batch_y[:, :-1])\n",
    "                        \n",
    "            loss = lossfunction(pred_y.view(-1, pred_y.shape[-1]), batch_y[:, 1:].contiguous().view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "                                         \n",
    "            normalization += (batch_y != 0).sum().item()\n",
    "            \n",
    "        train_loss = train_loss / normalization\n",
    "        list_train_loss.append(train_loss)\n",
    "        list_epoch.append(epoch)\n",
    "                \n",
    "        print(f'{time.time() - t0} seconds')\n",
    "        \n",
    "        if train_loss < best_loss:\n",
    "            print(f\"Epoch {epoch}: The best loss decreased from {best_loss: .4f} to {train_loss: .4f}\")\n",
    "            best_loss = train_loss\n",
    "            torch.save(model.state_dict(), './model/model_dict_best.pt')\n",
    "        else:\n",
    "            print(f\"Epoch {epoch}: The best train loss did not decrease\")\n",
    "        \n",
    "        print(f'Epoch: {epoch}, Train Loss: {train_loss: .4f}')\n",
    "        print('~' * 100)\n",
    "        \n",
    "    return list_epoch, list_train_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embedder): Embedding(159, 128)\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (k_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (v_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (k_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (v_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (k_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (v_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (k_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (v_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "            (k_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "            (v_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "            (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (k_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (v_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "            (k_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "            (v_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "            (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): DecoderBlock(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (q_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (k_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (v_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "          (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (transformer_block): TransformerBlock(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "            (k_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "            (v_linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "            (fc_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (fc_out): Linear(in_features=128, out_features=159, bias=True)\n",
      ")\n",
      "0.21030187606811523 seconds\n",
      "Epoch 0: The best loss decreased from  10000.0000 to  4.7539\n",
      "Epoch: 0, Train Loss:  4.7539\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.194746732711792 seconds\n",
      "Epoch 1: The best loss decreased from  4.7539 to  4.3269\n",
      "Epoch: 1, Train Loss:  4.3269\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.17851996421813965 seconds\n",
      "Epoch 2: The best loss decreased from  4.3269 to  4.1576\n",
      "Epoch: 2, Train Loss:  4.1576\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19333982467651367 seconds\n",
      "Epoch 3: The best loss decreased from  4.1576 to  4.0452\n",
      "Epoch: 3, Train Loss:  4.0452\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20363903045654297 seconds\n",
      "Epoch 4: The best loss decreased from  4.0452 to  3.9298\n",
      "Epoch: 4, Train Loss:  3.9298\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19407320022583008 seconds\n",
      "Epoch 5: The best loss decreased from  3.9298 to  3.7209\n",
      "Epoch: 5, Train Loss:  3.7209\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18085193634033203 seconds\n",
      "Epoch 6: The best loss decreased from  3.7209 to  3.5663\n",
      "Epoch: 6, Train Loss:  3.5663\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.193267822265625 seconds\n",
      "Epoch 7: The best loss decreased from  3.5663 to  3.3611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train Loss:  3.3611\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1924450397491455 seconds\n",
      "Epoch 8: The best loss decreased from  3.3611 to  3.1417\n",
      "Epoch: 8, Train Loss:  3.1417\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18451285362243652 seconds\n",
      "Epoch 9: The best loss decreased from  3.1417 to  2.9063\n",
      "Epoch: 9, Train Loss:  2.9063\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.186614990234375 seconds\n",
      "Epoch 10: The best loss decreased from  2.9063 to  2.8251\n",
      "Epoch: 10, Train Loss:  2.8251\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21060824394226074 seconds\n",
      "Epoch 11: The best loss decreased from  2.8251 to  2.5341\n",
      "Epoch: 11, Train Loss:  2.5341\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.2149341106414795 seconds\n",
      "Epoch 12: The best loss decreased from  2.5341 to  2.4290\n",
      "Epoch: 12, Train Loss:  2.4290\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21648287773132324 seconds\n",
      "Epoch 13: The best loss decreased from  2.4290 to  2.2758\n",
      "Epoch: 13, Train Loss:  2.2758\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21714305877685547 seconds\n",
      "Epoch 14: The best loss decreased from  2.2758 to  2.1994\n",
      "Epoch: 14, Train Loss:  2.1994\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.2184460163116455 seconds\n",
      "Epoch 15: The best loss decreased from  2.1994 to  1.9941\n",
      "Epoch: 15, Train Loss:  1.9941\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.2141399383544922 seconds\n",
      "Epoch 16: The best loss decreased from  1.9941 to  1.8671\n",
      "Epoch: 16, Train Loss:  1.8671\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21024012565612793 seconds\n",
      "Epoch 17: The best loss decreased from  1.8671 to  1.7776\n",
      "Epoch: 17, Train Loss:  1.7776\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.2178959846496582 seconds\n",
      "Epoch 18: The best loss decreased from  1.7776 to  1.6580\n",
      "Epoch: 18, Train Loss:  1.6580\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1963062286376953 seconds\n",
      "Epoch 19: The best loss decreased from  1.6580 to  1.5718\n",
      "Epoch: 19, Train Loss:  1.5718\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19285798072814941 seconds\n",
      "Epoch 20: The best loss decreased from  1.5718 to  1.4715\n",
      "Epoch: 20, Train Loss:  1.4715\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.17772221565246582 seconds\n",
      "Epoch 21: The best loss decreased from  1.4715 to  1.3756\n",
      "Epoch: 21, Train Loss:  1.3756\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1843099594116211 seconds\n",
      "Epoch 22: The best loss decreased from  1.3756 to  1.2782\n",
      "Epoch: 22, Train Loss:  1.2782\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1910710334777832 seconds\n",
      "Epoch 23: The best loss decreased from  1.2782 to  1.1945\n",
      "Epoch: 23, Train Loss:  1.1945\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18489718437194824 seconds\n",
      "Epoch 24: The best loss decreased from  1.1945 to  1.1205\n",
      "Epoch: 24, Train Loss:  1.1205\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.17621183395385742 seconds\n",
      "Epoch 25: The best loss decreased from  1.1205 to  1.0598\n",
      "Epoch: 25, Train Loss:  1.0598\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18344688415527344 seconds\n",
      "Epoch 26: The best loss decreased from  1.0598 to  1.0022\n",
      "Epoch: 26, Train Loss:  1.0022\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19452595710754395 seconds\n",
      "Epoch 27: The best loss decreased from  1.0022 to  0.9345\n",
      "Epoch: 27, Train Loss:  0.9345\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.25052881240844727 seconds\n",
      "Epoch 28: The best loss decreased from  0.9345 to  0.9261\n",
      "Epoch: 28, Train Loss:  0.9261\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21555590629577637 seconds\n",
      "Epoch 29: The best loss decreased from  0.9261 to  0.8223\n",
      "Epoch: 29, Train Loss:  0.8223\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21571707725524902 seconds\n",
      "Epoch 30: The best loss decreased from  0.8223 to  0.7293\n",
      "Epoch: 30, Train Loss:  0.7293\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21861791610717773 seconds\n",
      "Epoch 31: The best loss decreased from  0.7293 to  0.7152\n",
      "Epoch: 31, Train Loss:  0.7152\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19089794158935547 seconds\n",
      "Epoch 32: The best loss decreased from  0.7152 to  0.6528\n",
      "Epoch: 32, Train Loss:  0.6528\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1818859577178955 seconds\n",
      "Epoch 33: The best train loss did not decrease\n",
      "Epoch: 33, Train Loss:  0.6920\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21837997436523438 seconds\n",
      "Epoch 34: The best loss decreased from  0.6528 to  0.5952\n",
      "Epoch: 34, Train Loss:  0.5952\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19223594665527344 seconds\n",
      "Epoch 35: The best loss decreased from  0.5952 to  0.5775\n",
      "Epoch: 35, Train Loss:  0.5775\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20333003997802734 seconds\n",
      "Epoch 36: The best loss decreased from  0.5775 to  0.5542\n",
      "Epoch: 36, Train Loss:  0.5542\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1830899715423584 seconds\n",
      "Epoch 37: The best train loss did not decrease\n",
      "Epoch: 37, Train Loss:  0.5716\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20809698104858398 seconds\n",
      "Epoch 38: The best loss decreased from  0.5542 to  0.5370\n",
      "Epoch: 38, Train Loss:  0.5370\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.2114090919494629 seconds\n",
      "Epoch 39: The best loss decreased from  0.5370 to  0.4195\n",
      "Epoch: 39, Train Loss:  0.4195\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19917082786560059 seconds\n",
      "Epoch 40: The best train loss did not decrease\n",
      "Epoch: 40, Train Loss:  0.4363\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21366500854492188 seconds\n",
      "Epoch 41: The best loss decreased from  0.4195 to  0.4045\n",
      "Epoch: 41, Train Loss:  0.4045\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21377086639404297 seconds\n",
      "Epoch 42: The best loss decreased from  0.4045 to  0.3556\n",
      "Epoch: 42, Train Loss:  0.3556\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1799609661102295 seconds\n",
      "Epoch 43: The best loss decreased from  0.3556 to  0.3247\n",
      "Epoch: 43, Train Loss:  0.3247\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1959233283996582 seconds\n",
      "Epoch 44: The best train loss did not decrease\n",
      "Epoch: 44, Train Loss:  0.3604\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19440197944641113 seconds\n",
      "Epoch 45: The best loss decreased from  0.3247 to  0.2979\n",
      "Epoch: 45, Train Loss:  0.2979\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20883798599243164 seconds\n",
      "Epoch 46: The best loss decreased from  0.2979 to  0.2658\n",
      "Epoch: 46, Train Loss:  0.2658\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1955409049987793 seconds\n",
      "Epoch 47: The best loss decreased from  0.2658 to  0.2650\n",
      "Epoch: 47, Train Loss:  0.2650\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19533586502075195 seconds\n",
      "Epoch 48: The best loss decreased from  0.2650 to  0.2480\n",
      "Epoch: 48, Train Loss:  0.2480\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19196319580078125 seconds\n",
      "Epoch 49: The best train loss did not decrease\n",
      "Epoch: 49, Train Loss:  0.2644\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19572210311889648 seconds\n",
      "Epoch 50: The best loss decreased from  0.2480 to  0.2328\n",
      "Epoch: 50, Train Loss:  0.2328\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1951611042022705 seconds\n",
      "Epoch 51: The best loss decreased from  0.2328 to  0.2129\n",
      "Epoch: 51, Train Loss:  0.2129\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19231820106506348 seconds\n",
      "Epoch 52: The best loss decreased from  0.2129 to  0.1849\n",
      "Epoch: 52, Train Loss:  0.1849\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18596911430358887 seconds\n",
      "Epoch 53: The best train loss did not decrease\n",
      "Epoch: 53, Train Loss:  0.1949\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18319177627563477 seconds\n",
      "Epoch 54: The best loss decreased from  0.1849 to  0.1773\n",
      "Epoch: 54, Train Loss:  0.1773\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1919569969177246 seconds\n",
      "Epoch 55: The best loss decreased from  0.1773 to  0.1690\n",
      "Epoch: 55, Train Loss:  0.1690\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18229079246520996 seconds\n",
      "Epoch 56: The best loss decreased from  0.1690 to  0.1669\n",
      "Epoch: 56, Train Loss:  0.1669\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1862499713897705 seconds\n",
      "Epoch 57: The best train loss did not decrease\n",
      "Epoch: 57, Train Loss:  0.1764\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19565582275390625 seconds\n",
      "Epoch 58: The best loss decreased from  0.1669 to  0.1531\n",
      "Epoch: 58, Train Loss:  0.1531\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19217514991760254 seconds\n",
      "Epoch 59: The best train loss did not decrease\n",
      "Epoch: 59, Train Loss:  0.1534\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1931138038635254 seconds\n",
      "Epoch 60: The best loss decreased from  0.1531 to  0.1503\n",
      "Epoch: 60, Train Loss:  0.1503\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21382880210876465 seconds\n",
      "Epoch 61: The best loss decreased from  0.1503 to  0.1234\n",
      "Epoch: 61, Train Loss:  0.1234\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20250320434570312 seconds\n",
      "Epoch 62: The best train loss did not decrease\n",
      "Epoch: 62, Train Loss:  0.1398\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19227218627929688 seconds\n",
      "Epoch 63: The best loss decreased from  0.1234 to  0.1213\n",
      "Epoch: 63, Train Loss:  0.1213\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19469213485717773 seconds\n",
      "Epoch 64: The best train loss did not decrease\n",
      "Epoch: 64, Train Loss:  0.1227\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18678665161132812 seconds\n",
      "Epoch 65: The best loss decreased from  0.1213 to  0.1194\n",
      "Epoch: 65, Train Loss:  0.1194\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18554282188415527 seconds\n",
      "Epoch 66: The best loss decreased from  0.1194 to  0.1026\n",
      "Epoch: 66, Train Loss:  0.1026\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19129705429077148 seconds\n",
      "Epoch 67: The best train loss did not decrease\n",
      "Epoch: 67, Train Loss:  0.1041\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19731402397155762 seconds\n",
      "Epoch 68: The best loss decreased from  0.1026 to  0.0962\n",
      "Epoch: 68, Train Loss:  0.0962\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20077896118164062 seconds\n",
      "Epoch 69: The best train loss did not decrease\n",
      "Epoch: 69, Train Loss:  0.1060\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1904151439666748 seconds\n",
      "Epoch 70: The best loss decreased from  0.0962 to  0.0884\n",
      "Epoch: 70, Train Loss:  0.0884\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19803500175476074 seconds\n",
      "Epoch 71: The best train loss did not decrease\n",
      "Epoch: 71, Train Loss:  0.0891\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19501686096191406 seconds\n",
      "Epoch 72: The best train loss did not decrease\n",
      "Epoch: 72, Train Loss:  0.0946\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1958160400390625 seconds\n",
      "Epoch 73: The best train loss did not decrease\n",
      "Epoch: 73, Train Loss:  0.0952\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18758821487426758 seconds\n",
      "Epoch 74: The best train loss did not decrease\n",
      "Epoch: 74, Train Loss:  0.0910\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18767094612121582 seconds\n",
      "Epoch 75: The best loss decreased from  0.0884 to  0.0839\n",
      "Epoch: 75, Train Loss:  0.0839\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18415403366088867 seconds\n",
      "Epoch 76: The best train loss did not decrease\n",
      "Epoch: 76, Train Loss:  0.0920\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19681668281555176 seconds\n",
      "Epoch 77: The best loss decreased from  0.0839 to  0.0822\n",
      "Epoch: 77, Train Loss:  0.0822\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19487309455871582 seconds\n",
      "Epoch 78: The best loss decreased from  0.0822 to  0.0757\n",
      "Epoch: 78, Train Loss:  0.0757\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19867396354675293 seconds\n",
      "Epoch 79: The best train loss did not decrease\n",
      "Epoch: 79, Train Loss:  0.0908\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19276905059814453 seconds\n",
      "Epoch 80: The best loss decreased from  0.0757 to  0.0703\n",
      "Epoch: 80, Train Loss:  0.0703\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19478297233581543 seconds\n",
      "Epoch 81: The best train loss did not decrease\n",
      "Epoch: 81, Train Loss:  0.0871\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.185805082321167 seconds\n",
      "Epoch 82: The best train loss did not decrease\n",
      "Epoch: 82, Train Loss:  0.0851\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19775605201721191 seconds\n",
      "Epoch 83: The best train loss did not decrease\n",
      "Epoch: 83, Train Loss:  0.0785\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20102691650390625 seconds\n",
      "Epoch 84: The best train loss did not decrease\n",
      "Epoch: 84, Train Loss:  0.0961\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18829607963562012 seconds\n",
      "Epoch 85: The best train loss did not decrease\n",
      "Epoch: 85, Train Loss:  0.0741\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19643497467041016 seconds\n",
      "Epoch 86: The best train loss did not decrease\n",
      "Epoch: 86, Train Loss:  0.0728\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18559908866882324 seconds\n",
      "Epoch 87: The best loss decreased from  0.0703 to  0.0692\n",
      "Epoch: 87, Train Loss:  0.0692\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19877409934997559 seconds\n",
      "Epoch 88: The best loss decreased from  0.0692 to  0.0494\n",
      "Epoch: 88, Train Loss:  0.0494\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18838906288146973 seconds\n",
      "Epoch 89: The best train loss did not decrease\n",
      "Epoch: 89, Train Loss:  0.0634\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19220685958862305 seconds\n",
      "Epoch 90: The best train loss did not decrease\n",
      "Epoch: 90, Train Loss:  0.0503\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19966888427734375 seconds\n",
      "Epoch 91: The best train loss did not decrease\n",
      "Epoch: 91, Train Loss:  0.0578\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20044708251953125 seconds\n",
      "Epoch 92: The best loss decreased from  0.0494 to  0.0463\n",
      "Epoch: 92, Train Loss:  0.0463\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1989729404449463 seconds\n",
      "Epoch 93: The best loss decreased from  0.0463 to  0.0431\n",
      "Epoch: 93, Train Loss:  0.0431\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19212698936462402 seconds\n",
      "Epoch 94: The best train loss did not decrease\n",
      "Epoch: 94, Train Loss:  0.0468\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19710803031921387 seconds\n",
      "Epoch 95: The best loss decreased from  0.0431 to  0.0408\n",
      "Epoch: 95, Train Loss:  0.0408\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18559694290161133 seconds\n",
      "Epoch 96: The best loss decreased from  0.0408 to  0.0356\n",
      "Epoch: 96, Train Loss:  0.0356\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.2111508846282959 seconds\n",
      "Epoch 97: The best train loss did not decrease\n",
      "Epoch: 97, Train Loss:  0.0386\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18785381317138672 seconds\n",
      "Epoch 98: The best train loss did not decrease\n",
      "Epoch: 98, Train Loss:  0.0421\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19337701797485352 seconds\n",
      "Epoch 99: The best loss decreased from  0.0356 to  0.0336\n",
      "Epoch: 99, Train Loss:  0.0336\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1876220703125 seconds\n",
      "Epoch 100: The best train loss did not decrease\n",
      "Epoch: 100, Train Loss:  0.0396\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.2023320198059082 seconds\n",
      "Epoch 101: The best loss decreased from  0.0336 to  0.0333\n",
      "Epoch: 101, Train Loss:  0.0333\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1958470344543457 seconds\n",
      "Epoch 102: The best loss decreased from  0.0333 to  0.0305\n",
      "Epoch: 102, Train Loss:  0.0305\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1872718334197998 seconds\n",
      "Epoch 103: The best train loss did not decrease\n",
      "Epoch: 103, Train Loss:  0.0353\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1876201629638672 seconds\n",
      "Epoch 104: The best train loss did not decrease\n",
      "Epoch: 104, Train Loss:  0.0368\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20177602767944336 seconds\n",
      "Epoch 105: The best train loss did not decrease\n",
      "Epoch: 105, Train Loss:  0.0331\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19443607330322266 seconds\n",
      "Epoch 106: The best train loss did not decrease\n",
      "Epoch: 106, Train Loss:  0.0345\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19104290008544922 seconds\n",
      "Epoch 107: The best train loss did not decrease\n",
      "Epoch: 107, Train Loss:  0.0367\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1844627857208252 seconds\n",
      "Epoch 108: The best train loss did not decrease\n",
      "Epoch: 108, Train Loss:  0.0387\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1965019702911377 seconds\n",
      "Epoch 109: The best train loss did not decrease\n",
      "Epoch: 109, Train Loss:  0.0341\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19801616668701172 seconds\n",
      "Epoch 110: The best train loss did not decrease\n",
      "Epoch: 110, Train Loss:  0.0343\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20337891578674316 seconds\n",
      "Epoch 111: The best train loss did not decrease\n",
      "Epoch: 111, Train Loss:  0.0577\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1858057975769043 seconds\n",
      "Epoch 112: The best train loss did not decrease\n",
      "Epoch: 112, Train Loss:  0.0467\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19698190689086914 seconds\n",
      "Epoch 113: The best train loss did not decrease\n",
      "Epoch: 113, Train Loss:  0.0384\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18172788619995117 seconds\n",
      "Epoch 114: The best train loss did not decrease\n",
      "Epoch: 114, Train Loss:  0.0328\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1913151741027832 seconds\n",
      "Epoch 115: The best train loss did not decrease\n",
      "Epoch: 115, Train Loss:  0.0374\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18971705436706543 seconds\n",
      "Epoch 116: The best train loss did not decrease\n",
      "Epoch: 116, Train Loss:  0.0306\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19441795349121094 seconds\n",
      "Epoch 117: The best train loss did not decrease\n",
      "Epoch: 117, Train Loss:  0.0421\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18525195121765137 seconds\n",
      "Epoch 118: The best train loss did not decrease\n",
      "Epoch: 118, Train Loss:  0.0397\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19219303131103516 seconds\n",
      "Epoch 119: The best train loss did not decrease\n",
      "Epoch: 119, Train Loss:  0.0317\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18511104583740234 seconds\n",
      "Epoch 120: The best train loss did not decrease\n",
      "Epoch: 120, Train Loss:  0.0440\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20891094207763672 seconds\n",
      "Epoch 121: The best loss decreased from  0.0305 to  0.0283\n",
      "Epoch: 121, Train Loss:  0.0283\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1940150260925293 seconds\n",
      "Epoch 122: The best train loss did not decrease\n",
      "Epoch: 122, Train Loss:  0.0366\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20450186729431152 seconds\n",
      "Epoch 123: The best train loss did not decrease\n",
      "Epoch: 123, Train Loss:  0.0309\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.204909086227417 seconds\n",
      "Epoch 124: The best train loss did not decrease\n",
      "Epoch: 124, Train Loss:  0.0354\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21694183349609375 seconds\n",
      "Epoch 125: The best train loss did not decrease\n",
      "Epoch: 125, Train Loss:  0.0322\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.2293081283569336 seconds\n",
      "Epoch 126: The best train loss did not decrease\n",
      "Epoch: 126, Train Loss:  0.0308\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19786691665649414 seconds\n",
      "Epoch 127: The best train loss did not decrease\n",
      "Epoch: 127, Train Loss:  0.0346\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1996150016784668 seconds\n",
      "Epoch 128: The best loss decreased from  0.0283 to  0.0259\n",
      "Epoch: 128, Train Loss:  0.0259\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20433902740478516 seconds\n",
      "Epoch 129: The best loss decreased from  0.0259 to  0.0224\n",
      "Epoch: 129, Train Loss:  0.0224\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19747281074523926 seconds\n",
      "Epoch 130: The best train loss did not decrease\n",
      "Epoch: 130, Train Loss:  0.0296\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1936511993408203 seconds\n",
      "Epoch 131: The best train loss did not decrease\n",
      "Epoch: 131, Train Loss:  0.0255\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20252704620361328 seconds\n",
      "Epoch 132: The best train loss did not decrease\n",
      "Epoch: 132, Train Loss:  0.0276\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18552303314208984 seconds\n",
      "Epoch 133: The best train loss did not decrease\n",
      "Epoch: 133, Train Loss:  0.0234\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19684672355651855 seconds\n",
      "Epoch 134: The best loss decreased from  0.0224 to  0.0223\n",
      "Epoch: 134, Train Loss:  0.0223\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19051790237426758 seconds\n",
      "Epoch 135: The best loss decreased from  0.0223 to  0.0179\n",
      "Epoch: 135, Train Loss:  0.0179\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20471882820129395 seconds\n",
      "Epoch 136: The best train loss did not decrease\n",
      "Epoch: 136, Train Loss:  0.0190\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19824790954589844 seconds\n",
      "Epoch 137: The best loss decreased from  0.0179 to  0.0158\n",
      "Epoch: 137, Train Loss:  0.0158\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20410585403442383 seconds\n",
      "Epoch 138: The best loss decreased from  0.0158 to  0.0158\n",
      "Epoch: 138, Train Loss:  0.0158\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.2041921615600586 seconds\n",
      "Epoch 139: The best train loss did not decrease\n",
      "Epoch: 139, Train Loss:  0.0175\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19592618942260742 seconds\n",
      "Epoch 140: The best train loss did not decrease\n",
      "Epoch: 140, Train Loss:  0.0169\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18384599685668945 seconds\n",
      "Epoch 141: The best loss decreased from  0.0158 to  0.0151\n",
      "Epoch: 141, Train Loss:  0.0151\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.17747783660888672 seconds\n",
      "Epoch 142: The best train loss did not decrease\n",
      "Epoch: 142, Train Loss:  0.0156\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19135212898254395 seconds\n",
      "Epoch 143: The best train loss did not decrease\n",
      "Epoch: 143, Train Loss:  0.0178\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18274307250976562 seconds\n",
      "Epoch 144: The best train loss did not decrease\n",
      "Epoch: 144, Train Loss:  0.0154\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20237493515014648 seconds\n",
      "Epoch 145: The best loss decreased from  0.0151 to  0.0143\n",
      "Epoch: 145, Train Loss:  0.0143\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.17681598663330078 seconds\n",
      "Epoch 146: The best train loss did not decrease\n",
      "Epoch: 146, Train Loss:  0.0183\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1951758861541748 seconds\n",
      "Epoch 147: The best train loss did not decrease\n",
      "Epoch: 147, Train Loss:  0.0149\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1876678466796875 seconds\n",
      "Epoch 148: The best train loss did not decrease\n",
      "Epoch: 148, Train Loss:  0.0156\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19654226303100586 seconds\n",
      "Epoch 149: The best train loss did not decrease\n",
      "Epoch: 149, Train Loss:  0.0150\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18781709671020508 seconds\n",
      "Epoch 150: The best loss decreased from  0.0143 to  0.0114\n",
      "Epoch: 150, Train Loss:  0.0114\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19741606712341309 seconds\n",
      "Epoch 151: The best train loss did not decrease\n",
      "Epoch: 151, Train Loss:  0.0153\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18863701820373535 seconds\n",
      "Epoch 152: The best train loss did not decrease\n",
      "Epoch: 152, Train Loss:  0.0145\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18474078178405762 seconds\n",
      "Epoch 153: The best train loss did not decrease\n",
      "Epoch: 153, Train Loss:  0.0154\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.23175406455993652 seconds\n",
      "Epoch 154: The best train loss did not decrease\n",
      "Epoch: 154, Train Loss:  0.0159\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20569491386413574 seconds\n",
      "Epoch 155: The best train loss did not decrease\n",
      "Epoch: 155, Train Loss:  0.0144\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19399499893188477 seconds\n",
      "Epoch 156: The best train loss did not decrease\n",
      "Epoch: 156, Train Loss:  0.0178\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19862985610961914 seconds\n",
      "Epoch 157: The best train loss did not decrease\n",
      "Epoch: 157, Train Loss:  0.0147\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18860292434692383 seconds\n",
      "Epoch 158: The best train loss did not decrease\n",
      "Epoch: 158, Train Loss:  0.0200\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21310901641845703 seconds\n",
      "Epoch 159: The best train loss did not decrease\n",
      "Epoch: 159, Train Loss:  0.0122\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20751690864562988 seconds\n",
      "Epoch 160: The best train loss did not decrease\n",
      "Epoch: 160, Train Loss:  0.0258\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21372580528259277 seconds\n",
      "Epoch 161: The best train loss did not decrease\n",
      "Epoch: 161, Train Loss:  0.0165\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19408297538757324 seconds\n",
      "Epoch 162: The best train loss did not decrease\n",
      "Epoch: 162, Train Loss:  0.0198\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18819522857666016 seconds\n",
      "Epoch 163: The best train loss did not decrease\n",
      "Epoch: 163, Train Loss:  0.0151\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19173979759216309 seconds\n",
      "Epoch 164: The best train loss did not decrease\n",
      "Epoch: 164, Train Loss:  0.0189\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2010331153869629 seconds\n",
      "Epoch 165: The best train loss did not decrease\n",
      "Epoch: 165, Train Loss:  0.0161\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20267891883850098 seconds\n",
      "Epoch 166: The best train loss did not decrease\n",
      "Epoch: 166, Train Loss:  0.0246\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20230603218078613 seconds\n",
      "Epoch 167: The best train loss did not decrease\n",
      "Epoch: 167, Train Loss:  0.0202\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.2063608169555664 seconds\n",
      "Epoch 168: The best train loss did not decrease\n",
      "Epoch: 168, Train Loss:  0.0151\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20043015480041504 seconds\n",
      "Epoch 169: The best train loss did not decrease\n",
      "Epoch: 169, Train Loss:  0.0151\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19445300102233887 seconds\n",
      "Epoch 170: The best train loss did not decrease\n",
      "Epoch: 170, Train Loss:  0.0148\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19469904899597168 seconds\n",
      "Epoch 171: The best train loss did not decrease\n",
      "Epoch: 171, Train Loss:  0.0115\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20206809043884277 seconds\n",
      "Epoch 172: The best train loss did not decrease\n",
      "Epoch: 172, Train Loss:  0.0172\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21244597434997559 seconds\n",
      "Epoch 173: The best train loss did not decrease\n",
      "Epoch: 173, Train Loss:  0.0190\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20478105545043945 seconds\n",
      "Epoch 174: The best train loss did not decrease\n",
      "Epoch: 174, Train Loss:  0.0167\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1979660987854004 seconds\n",
      "Epoch 175: The best train loss did not decrease\n",
      "Epoch: 175, Train Loss:  0.0148\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.2024400234222412 seconds\n",
      "Epoch 176: The best train loss did not decrease\n",
      "Epoch: 176, Train Loss:  0.0151\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18327808380126953 seconds\n",
      "Epoch 177: The best train loss did not decrease\n",
      "Epoch: 177, Train Loss:  0.0120\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1959989070892334 seconds\n",
      "Epoch 178: The best train loss did not decrease\n",
      "Epoch: 178, Train Loss:  0.0243\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18639183044433594 seconds\n",
      "Epoch 179: The best train loss did not decrease\n",
      "Epoch: 179, Train Loss:  0.0163\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.2038431167602539 seconds\n",
      "Epoch 180: The best train loss did not decrease\n",
      "Epoch: 180, Train Loss:  0.0130\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.18983697891235352 seconds\n",
      "Epoch 181: The best train loss did not decrease\n",
      "Epoch: 181, Train Loss:  0.0210\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21016287803649902 seconds\n",
      "Epoch 182: The best train loss did not decrease\n",
      "Epoch: 182, Train Loss:  0.0221\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1985180377960205 seconds\n",
      "Epoch 183: The best train loss did not decrease\n",
      "Epoch: 183, Train Loss:  0.0183\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20244884490966797 seconds\n",
      "Epoch 184: The best train loss did not decrease\n",
      "Epoch: 184, Train Loss:  0.0671\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21973228454589844 seconds\n",
      "Epoch 185: The best train loss did not decrease\n",
      "Epoch: 185, Train Loss:  0.0703\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21156024932861328 seconds\n",
      "Epoch 186: The best train loss did not decrease\n",
      "Epoch: 186, Train Loss:  0.0296\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21953701972961426 seconds\n",
      "Epoch 187: The best train loss did not decrease\n",
      "Epoch: 187, Train Loss:  0.0407\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19200706481933594 seconds\n",
      "Epoch 188: The best train loss did not decrease\n",
      "Epoch: 188, Train Loss:  0.0625\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.2025136947631836 seconds\n",
      "Epoch 189: The best train loss did not decrease\n",
      "Epoch: 189, Train Loss:  0.0378\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21393108367919922 seconds\n",
      "Epoch 190: The best train loss did not decrease\n",
      "Epoch: 190, Train Loss:  0.0231\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.21385407447814941 seconds\n",
      "Epoch 191: The best train loss did not decrease\n",
      "Epoch: 191, Train Loss:  0.0354\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19870305061340332 seconds\n",
      "Epoch 192: The best train loss did not decrease\n",
      "Epoch: 192, Train Loss:  0.0347\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20969605445861816 seconds\n",
      "Epoch 193: The best train loss did not decrease\n",
      "Epoch: 193, Train Loss:  0.0350\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20599699020385742 seconds\n",
      "Epoch 194: The best train loss did not decrease\n",
      "Epoch: 194, Train Loss:  0.0279\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1961808204650879 seconds\n",
      "Epoch 195: The best train loss did not decrease\n",
      "Epoch: 195, Train Loss:  0.0764\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.19543790817260742 seconds\n",
      "Epoch 196: The best train loss did not decrease\n",
      "Epoch: 196, Train Loss:  0.0912\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.1899571418762207 seconds\n",
      "Epoch 197: The best train loss did not decrease\n",
      "Epoch: 197, Train Loss:  0.0420\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20591068267822266 seconds\n",
      "Epoch 198: The best train loss did not decrease\n",
      "Epoch: 198, Train Loss:  0.0468\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "0.20346307754516602 seconds\n",
      "Epoch 199: The best train loss did not decrease\n",
      "Epoch: 199, Train Loss:  0.0770\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.epoch = 200\n",
    "args.batch_size = 5\n",
    "\n",
    "num_train = len(preprocessor.sequences[0])\n",
    "\n",
    "train_dataset = S2SDataset(preprocessor.sequences[0], preprocessor.sequences[1])\n",
    "\n",
    "list_epoch, list_train_loss = experiment(train_dataset, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "# ====== Plot Loss ====== #\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(list_epoch, list_train_loss, label='train_loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "ax1.set_title('epoch vs loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 832,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(preprocessor.num_tokens, 0, 128, 3, 4, 4, device = device, max_seq_len = preprocessor.max_seq_length)\n",
    "model.load_state_dict(torch.load('./model/model_dict_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(input_seq, model, SOS_idx, EOS_idx, PAD_idx, max_seq_len, method = 0):\n",
    "    '''\n",
    "            This function is used to generate a output sequence self-recursively according to \n",
    "        an input sequence.\n",
    "    '''\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    input_mask = (input_seq != PAD_idx).unsqueeze(1).unsqueeze(2).to(device)  # Make input mask\n",
    "    input_embedded = model.embedder(input_seq)\n",
    "    encoding = model.encoder(input_seq, input_embedded, input_mask)  # Encoder the input\n",
    "    decoder_input = torch.LongTensor([[SOS_idx]]).to(device)  # Use SOS token to initiate the decoder\n",
    "    \n",
    "    # Continue obtaining the next decoder token until decoder outputs an EOS token or it reaches max_len \n",
    "    for pos in range(max_seq_len):\n",
    "        # Make target mask, pos + 1 cause pos starts at 0\n",
    "        decoder_input_mask = (torch.triu(torch.ones((1, 1, pos + 1, pos + 1)), diagonal = 1) == 0).to(device) \n",
    "        \n",
    "        decoder_input_embedded = model.embedder(decoder_input)\n",
    "\n",
    "        out = model.fc_out(model.decoder(decoder_input, decoder_input_embedded, encoding, input_mask, decoder_input_mask))\n",
    "\n",
    "        probs = torch.softmax(out, dim = -1) \n",
    "        if method == 0:\n",
    "            action = torch.argmax(probs[:, -1]).unsqueeze(0).unsqueeze(0)\n",
    "        else:\n",
    "            # probs is a categorical probability distribution over the output vocab\n",
    "            distr = Categorical(probs = probs)\n",
    "            action = distr.sample()[:, -1].unsqueeze(0) # sample from that distribution to get next token\n",
    "\n",
    "        # Concatenate that token to our running list of output tokens \n",
    "        decoder_input = torch.cat((decoder_input, action), dim = 1) \n",
    "        # If the model outputs an EOS token, it is terminated\n",
    "        if action == EOS_idx:\n",
    "            # [0] because we assume batch size of 1 \n",
    "            # [1: -1] excludes the start and end token from the output string \n",
    "            de_str = decoder_input[0][1: -1]\n",
    "            return de_str\n",
    "        \n",
    "    de_str = decoder_input[0][1:]\n",
    "    return de_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the model predicts the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[hello i am jiayu what is your name]\n",
      "True: i am dennis nice to meet you\n",
      "Pred: i am dennis nice to meet you\n",
      "\n",
      "\n",
      "[how was the party yesterday]\n",
      "True: it was super nice everybody had fun\n",
      "Pred: it was super nice everybody had fun\n",
      "\n",
      "\n",
      "[when do you usually go to the office in the morning]\n",
      "True: i usually go to the office at 9 but i was late this morning\n",
      "Pred: i usually go to the office at 9 but i\n",
      "\n",
      "\n",
      "[do you want to meet in the campus at 10 pm tonight]\n",
      "True: okay that sounds good i will meet you there tonight\n",
      "Pred: okay that sounds good i will meet you there tonight\n",
      "\n",
      "\n",
      "[does this sunday works for everyone]\n",
      "True: yes it works perferct for me\n",
      "Pred: yes it works perferct for me\n",
      "\n",
      "\n",
      "[what is your favourite book of all time]\n",
      "True: three body problem i love it so much and i have read it twice\n",
      "Pred: three body problem i love it so much and i\n",
      "\n",
      "\n",
      "[what do you love doing for fun at spare time]\n",
      "True: i play soccer and go hiking and i sometimes go swimming too\n",
      "Pred: i play soccer and go hiking and i sometimes go\n",
      "\n",
      "\n",
      "[how was the exam last week was it hard]\n",
      "True: it was quite hard actually i did not do it well\n",
      "Pred: it was quite hard actually i did not do it\n",
      "\n",
      "\n",
      "[hope you could enjoy your trip next week]\n",
      "True: thanks i am sure i will\n",
      "Pred: thanks i am sure i will\n",
      "\n",
      "\n",
      "[is it going to rain later today]\n",
      "True: yes i think so do not forget your umbrella\n",
      "Pred: yes i think so do not forget your umbrella\n",
      "\n",
      "\n",
      "[so what are your plans for this weekend]\n",
      "True: i do not know do you want to get together or something\n",
      "Pred: i do not know do you want to get together\n",
      "\n",
      "\n",
      "[i do not know do you want to get together or something]\n",
      "True: how about going to see a movie\n",
      "Pred: how about going to see a movie\n",
      "\n",
      "\n",
      "[how about going to see a movie]\n",
      "True: that sounds like a good idea maybe we should go out to eat beforehand\n",
      "Pred: that sounds like a good idea maybe we should go\n",
      "\n",
      "\n",
      "[that sounds like a good idea maybe we should go out to eat beforehand]\n",
      "True: it is fine with me where do you want to meet\n",
      "Pred: it is fine with me where do you want to\n",
      "\n",
      "\n",
      "[it is fine with me where do you want to meet]\n",
      "True: let us meet at summer pizza house i have not gone there for a long time\n",
      "Pred: let us meet at summer pizza house i have not\n",
      "\n",
      "\n",
      "[when should we meet]\n",
      "True: well the movie is shown at 2 pm how about we meet at 1 pm\n",
      "Pred: well the movie is shown at 2 pm how about\n",
      "\n",
      "\n",
      "[what is she doing these days]\n",
      "True: she graduated last june and she will start her teaching career next week\n",
      "Pred: she graduated last june and she will start her teaching\n",
      "\n",
      "\n",
      "[hi i have not seen you in a while]\n",
      "True: yes i know it has been a long time\n",
      "Pred: yes i know it has been a long time\n",
      "\n",
      "\n",
      "[what have you been doing lately]\n",
      "True: i have been going to graduate school\n",
      "Pred: i have been going to graduate school\n",
      "\n",
      "\n",
      "[what are you majoring in]\n",
      "True: i am studying theoretical physics it is hard\n",
      "Pred: i am studying theoretical physics it is hard\n",
      "\n",
      "\n",
      "[okay nice to see you today bye]\n",
      "True: yeah me too see you later bye\n",
      "Pred: yeah me too see you later bye\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(preprocessor.sequences[0])):\n",
    "\n",
    "    input_seq = torch.tensor([preprocessor.sequences[0][i]], dtype = torch.int64).to(device)\n",
    "\n",
    "    outputs = predict_sequence(input_seq, model, 1, 2, 0, 10)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    print(f\"[{' '.join([preprocessor.idx2word[token] for token in preprocessor.sequences[0][i][1:-1]])}]\")\n",
    "    print(f\"True: {' '.join([preprocessor.idx2word[token] for token in preprocessor.sequences[1][i][1:-1]])}\")\n",
    "    print(f\"Pred: {' '.join([preprocessor.idx2word[token] for token in outputs.numpy().tolist()])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to chat with the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Hey, I'm lucy!\n",
      "Bot: i am dennis nice to meet you\n",
      "\n",
      "\n",
      "You: what have you been up to?\n",
      "Bot: i have been going to graduate school\n",
      "\n",
      "\n",
      "You: let's meet at 10\n",
      "Bot: okay that sounds good i will meet you there tonight\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sentences = [[\"Hey, I'm lucy!\"], [\"what have you been up to?\"], [\"let's meet at 10\"]]\n",
    "\n",
    "input_cleaned = preprocessor.cleanse_corpus(input_sentences)\n",
    "\n",
    "input_seqs = preprocessor.get_sequences_of_test_texts(input_cleaned)\n",
    "\n",
    "for i in range(len(input_seqs)):\n",
    "    input_seq = [input_seqs[i]]\n",
    "    \n",
    "    input_seq = torch.tensor(input_seq, dtype = torch.int64).to(device)\n",
    "\n",
    "    output_seq = predict_sequence(input_seq, model, 1, 2, 0, 20).numpy()\n",
    "\n",
    "    words = []\n",
    "    for idx in output_seq:\n",
    "        words.append(preprocessor.idx2word[idx])\n",
    "    output_sentence = \" \".join(words)\n",
    "\n",
    "    print(f\"You: {input_sentences[i][0]}\")\n",
    "    print(f\"Bot: {output_sentence}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMsr5cT4nGEybYESEmFAYWl",
   "collapsed_sections": [],
   "name": "BBox Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
